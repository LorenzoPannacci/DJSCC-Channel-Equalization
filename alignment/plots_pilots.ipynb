{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e4bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/repos/Deep-JSCC-PyTorch\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from alignment.alignment_utils import load_deep_jscc\n",
    "from alignment.alignment_model import *\n",
    "from alignment.alignment_model import _LinearAlignment, _MLPAlignment, _ConvolutionalAlignment, _ZeroShotAlignment, _TwoConvAlignment\n",
    "from alignment.alignment_training import *\n",
    "from alignment.alignment_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4dce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching inputs: 100%|██████████| 782/782 [00:06<00:00, 129.04it/s]\n"
     ]
    }
   ],
   "source": [
    "model1_fp = r'alignment/models/autoencoders/snr_0_seed_42.pkl'\n",
    "model2_fp = r'alignment/models/autoencoders/snr_0_seed_43.pkl'\n",
    "folder = r'psnr_vs_pilots_5'\n",
    "os.makedirs(f'alignment/models/plots/{folder}', exist_ok=True)\n",
    "\n",
    "dataset = \"cifar10\"\n",
    "resolution = 96\n",
    "channel = 'AWGN'\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "train_snr = 0\n",
    "val_snr = 0\n",
    "times = 10\n",
    "c = 8\n",
    "seed = 42\n",
    "\n",
    "samples_sets = np.unique(np.logspace(0, np.log10(10000), num=100, base=10).astype(int))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = copy.deepcopy(load_deep_jscc(model1_fp, val_snr, c, \"AWGN\").encoder)\n",
    "decoder = copy.deepcopy(load_deep_jscc(model2_fp, val_snr, c, \"AWGN\").decoder)\n",
    "\n",
    "train_loader, test_loader = get_data_loaders(dataset, resolution, batch_size, num_workers)\n",
    "data = load_alignment_dataset(model1_fp, model2_fp, train_snr, train_loader, c, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea546b",
   "metadata": {},
   "source": [
    "# No mismatch - Unaligned - Zeroshot max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6306f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaligned 5.39\n"
     ]
    }
   ],
   "source": [
    "model = AlignedDeepJSCC(encoder, decoder, None, val_snr, \"AWGN\")\n",
    "print(f\"Unaligned {validation_vectorized(model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a0ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned 48.50\n"
     ]
    }
   ],
   "source": [
    "model = AlignedDeepJSCC(encoder, copy.deepcopy(load_deep_jscc(model1_fp, val_snr, c, \"AWGN\").decoder), None, val_snr, \"AWGN\")\n",
    "print(f\"Aligned {validation_vectorized(model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.flat = True\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "aligner = train_zeroshot_aligner(data, permutation, resolution**2, train_snr, resolution**2, device)\n",
    "aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "print(f\"Zeroshot max size {validation_vectorized(model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29cfc6",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039882c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1.\n",
      "Done with 2.\n",
      "Done with 3.\n",
      "Done with 4.\n",
      "Done with 5.\n",
      "Done with 6.\n",
      "Done with 7.\n",
      "Done with 8.\n",
      "Done with 9.\n",
      "Done with 10.\n",
      "Done with 11.\n",
      "Done with 12.\n",
      "Done with 13.\n",
      "Done with 14.\n",
      "Done with 16.\n",
      "Done with 17.\n",
      "Done with 19.\n",
      "Done with 21.\n",
      "Done with 23.\n",
      "Done with 25.\n",
      "Done with 28.\n",
      "Done with 31.\n",
      "Done with 34.\n",
      "Done with 37.\n",
      "Done with 41.\n",
      "Done with 45.\n",
      "Done with 49.\n",
      "Done with 54.\n",
      "Done with 59.\n",
      "Done with 65.\n",
      "Done with 72.\n",
      "Done with 79.\n",
      "Done with 86.\n",
      "Done with 95.\n",
      "Done with 104.\n",
      "Done with 114.\n",
      "Done with 126.\n",
      "Done with 138.\n",
      "Done with 151.\n",
      "Done with 166.\n",
      "Done with 183.\n",
      "Done with 200.\n",
      "Done with 220.\n",
      "Done with 242.\n",
      "Done with 265.\n",
      "Done with 291.\n",
      "Done with 319.\n",
      "Done with 351.\n",
      "Done with 385.\n",
      "Done with 422.\n",
      "Done with 464.\n",
      "Done with 509.\n",
      "Done with 559.\n",
      "Done with 613.\n",
      "Done with 673.\n",
      "Done with 739.\n",
      "Done with 811.\n",
      "Done with 890.\n",
      "Done with 977.\n",
      "Done with 1072.\n",
      "Done with 1176.\n",
      "Done with 1291.\n",
      "Done with 1417.\n",
      "Done with 1555.\n",
      "Done with 1707.\n",
      "Done with 1873.\n",
      "Done with 2056.\n",
      "Done with 2257.\n",
      "Done with 2477.\n",
      "Done with 2718.\n",
      "Done with 2983.\n",
      "Done with 3274.\n",
      "Done with 3593.\n",
      "Done with 3944.\n",
      "Done with 4328.\n",
      "Done with 4750.\n",
      "Done with 5214.\n",
      "Done with 5722.\n",
      "Done with 6280.\n",
      "Done with 6892.\n",
      "Done with 7564.\n",
      "Done with 8302.\n",
      "Done with 9111.\n",
      "Done with 10000.\n"
     ]
    }
   ],
   "source": [
    "data.flat = True\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "\n",
    "    aligner = train_linear_aligner(data, permutation, n_samples, train_snr)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_linear_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    print(f\"Done with {n_samples}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1dd758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model, 1 samples got a PSNR of 11.91\n",
      "Linear model, 2 samples got a PSNR of 12.02\n",
      "Linear model, 3 samples got a PSNR of 12.07\n",
      "Linear model, 4 samples got a PSNR of 12.22\n",
      "Linear model, 5 samples got a PSNR of 12.63\n",
      "Linear model, 6 samples got a PSNR of 12.72\n",
      "Linear model, 7 samples got a PSNR of 12.79\n",
      "Linear model, 8 samples got a PSNR of 12.84\n",
      "Linear model, 9 samples got a PSNR of 13.27\n",
      "Linear model, 10 samples got a PSNR of 13.25\n",
      "Linear model, 11 samples got a PSNR of 13.53\n",
      "Linear model, 12 samples got a PSNR of 13.63\n",
      "Linear model, 13 samples got a PSNR of 13.68\n",
      "Linear model, 14 samples got a PSNR of 13.76\n",
      "Linear model, 16 samples got a PSNR of 13.94\n",
      "Linear model, 17 samples got a PSNR of 14.00\n",
      "Linear model, 19 samples got a PSNR of 14.04\n",
      "Linear model, 21 samples got a PSNR of 14.40\n",
      "Linear model, 23 samples got a PSNR of 14.56\n",
      "Linear model, 25 samples got a PSNR of 14.66\n",
      "Linear model, 28 samples got a PSNR of 14.85\n",
      "Linear model, 31 samples got a PSNR of 14.99\n",
      "Linear model, 34 samples got a PSNR of 15.03\n",
      "Linear model, 37 samples got a PSNR of 15.09\n",
      "Linear model, 41 samples got a PSNR of 15.18\n",
      "Linear model, 45 samples got a PSNR of 15.32\n",
      "Linear model, 49 samples got a PSNR of 15.46\n",
      "Linear model, 54 samples got a PSNR of 15.55\n",
      "Linear model, 59 samples got a PSNR of 15.71\n",
      "Linear model, 65 samples got a PSNR of 15.84\n",
      "Linear model, 72 samples got a PSNR of 15.98\n",
      "Linear model, 79 samples got a PSNR of 16.08\n",
      "Linear model, 86 samples got a PSNR of 16.19\n",
      "Linear model, 95 samples got a PSNR of 16.36\n",
      "Linear model, 104 samples got a PSNR of 16.56\n",
      "Linear model, 114 samples got a PSNR of 16.76\n",
      "Linear model, 126 samples got a PSNR of 16.94\n",
      "Linear model, 138 samples got a PSNR of 17.14\n",
      "Linear model, 151 samples got a PSNR of 17.26\n",
      "Linear model, 166 samples got a PSNR of 17.46\n",
      "Linear model, 183 samples got a PSNR of 17.64\n",
      "Linear model, 200 samples got a PSNR of 17.84\n",
      "Linear model, 220 samples got a PSNR of 18.05\n",
      "Linear model, 242 samples got a PSNR of 18.28\n",
      "Linear model, 265 samples got a PSNR of 18.47\n",
      "Linear model, 291 samples got a PSNR of 18.67\n",
      "Linear model, 319 samples got a PSNR of 18.85\n",
      "Linear model, 351 samples got a PSNR of 19.07\n",
      "Linear model, 385 samples got a PSNR of 19.28\n",
      "Linear model, 422 samples got a PSNR of 19.50\n",
      "Linear model, 464 samples got a PSNR of 19.70\n",
      "Linear model, 509 samples got a PSNR of 19.90\n",
      "Linear model, 559 samples got a PSNR of 20.15\n",
      "Linear model, 613 samples got a PSNR of 20.36\n",
      "Linear model, 673 samples got a PSNR of 20.60\n",
      "Linear model, 739 samples got a PSNR of 20.84\n",
      "Linear model, 811 samples got a PSNR of 21.09\n",
      "Linear model, 890 samples got a PSNR of 21.34\n",
      "Linear model, 977 samples got a PSNR of 21.61\n",
      "Linear model, 1072 samples got a PSNR of 21.90\n",
      "Linear model, 1176 samples got a PSNR of 22.18\n",
      "Linear model, 1291 samples got a PSNR of 22.46\n",
      "Linear model, 1417 samples got a PSNR of 22.74\n",
      "Linear model, 1555 samples got a PSNR of 23.06\n",
      "Linear model, 1707 samples got a PSNR of 23.36\n",
      "Linear model, 1873 samples got a PSNR of 23.67\n",
      "Linear model, 2056 samples got a PSNR of 24.01\n",
      "Linear model, 2257 samples got a PSNR of 24.35\n",
      "Linear model, 2477 samples got a PSNR of 24.71\n",
      "Linear model, 2718 samples got a PSNR of 25.06\n",
      "Linear model, 2983 samples got a PSNR of 25.43\n",
      "Linear model, 3274 samples got a PSNR of 25.79\n",
      "Linear model, 3593 samples got a PSNR of 26.16\n",
      "Linear model, 3944 samples got a PSNR of 26.55\n",
      "Linear model, 4328 samples got a PSNR of 26.93\n",
      "Linear model, 4750 samples got a PSNR of 27.32\n",
      "Linear model, 5214 samples got a PSNR of 27.72\n",
      "Linear model, 5722 samples got a PSNR of 28.14\n",
      "Linear model, 6280 samples got a PSNR of 28.56\n",
      "Linear model, 6892 samples got a PSNR of 28.97\n",
      "Linear model, 7564 samples got a PSNR of 29.40\n",
      "Linear model, 8302 samples got a PSNR of 29.83\n",
      "Linear model, 9111 samples got a PSNR of 30.27\n",
      "Linear model, 10000 samples got a PSNR of 30.71\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "aligner = _LinearAlignment(resolution**2)\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_linear_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, channel)\n",
    "\n",
    "    print(f\"Linear model, {n_samples} samples got a PSNR of {validation_vectorized(aligned_model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f355d4",
   "metadata": {},
   "source": [
    "# Linear Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979343a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1. Trained for 18 epochs.\n",
      "Done with 2. Trained for 24 epochs.\n",
      "Done with 3. Trained for 31 epochs.\n",
      "Done with 4. Trained for 45 epochs.\n",
      "Done with 5. Trained for 38 epochs.\n",
      "Done with 6. Trained for 67 epochs.\n",
      "Done with 7. Trained for 84 epochs.\n",
      "Done with 8. Trained for 44 epochs.\n",
      "Done with 9. Trained for 157 epochs.\n",
      "Done with 10. Trained for 105 epochs.\n",
      "Done with 11. Trained for 146 epochs.\n",
      "Done with 12. Trained for 103 epochs.\n",
      "Done with 13. Trained for 102 epochs.\n",
      "Done with 14. Trained for 12 epochs.\n",
      "Done with 16. Trained for 12 epochs.\n",
      "Done with 17. Trained for 12 epochs.\n",
      "Done with 19. Trained for 12 epochs.\n",
      "Done with 21. Trained for 12 epochs.\n",
      "Done with 23. Trained for 12 epochs.\n",
      "Done with 25. Trained for 12 epochs.\n",
      "Done with 28. Trained for 424 epochs.\n",
      "Done with 31. Trained for 599 epochs.\n",
      "Done with 34. Trained for 505 epochs.\n",
      "Done with 37. Trained for 614 epochs.\n",
      "Done with 41. Trained for 667 epochs.\n",
      "Done with 45. Trained for 723 epochs.\n",
      "Done with 49. Trained for 687 epochs.\n",
      "Done with 54. Trained for 626 epochs.\n",
      "Done with 59. Trained for 680 epochs.\n",
      "Done with 65. Trained for 414 epochs.\n",
      "Done with 72. Trained for 357 epochs.\n",
      "Done with 79. Trained for 363 epochs.\n",
      "Done with 86. Trained for 352 epochs.\n",
      "Done with 95. Trained for 378 epochs.\n",
      "Done with 104. Trained for 355 epochs.\n",
      "Done with 114. Trained for 350 epochs.\n",
      "Done with 126. Trained for 324 epochs.\n",
      "Done with 138. Trained for 247 epochs.\n",
      "Done with 151. Trained for 244 epochs.\n",
      "Done with 166. Trained for 229 epochs.\n",
      "Done with 183. Trained for 213 epochs.\n",
      "Done with 200. Trained for 193 epochs.\n",
      "Done with 220. Trained for 171 epochs.\n",
      "Done with 242. Trained for 166 epochs.\n",
      "Done with 265. Trained for 131 epochs.\n",
      "Done with 291. Trained for 132 epochs.\n",
      "Done with 319. Trained for 129 epochs.\n",
      "Done with 351. Trained for 90 epochs.\n",
      "Done with 385. Trained for 61 epochs.\n",
      "Done with 422. Trained for 83 epochs.\n",
      "Done with 464. Trained for 56 epochs.\n",
      "Done with 509. Trained for 77 epochs.\n",
      "Done with 559. Trained for 71 epochs.\n",
      "Done with 613. Trained for 63 epochs.\n",
      "Done with 673. Trained for 55 epochs.\n",
      "Done with 739. Trained for 57 epochs.\n",
      "Done with 811. Trained for 52 epochs.\n",
      "Done with 890. Trained for 49 epochs.\n",
      "Done with 977. Trained for 43 epochs.\n",
      "Done with 1072. Trained for 40 epochs.\n",
      "Done with 1176. Trained for 38 epochs.\n",
      "Done with 1291. Trained for 35 epochs.\n",
      "Done with 1417. Trained for 31 epochs.\n",
      "Done with 1555. Trained for 30 epochs.\n",
      "Done with 1707. Trained for 28 epochs.\n",
      "Done with 1873. Trained for 27 epochs.\n",
      "Done with 2056. Trained for 26 epochs.\n",
      "Done with 2257. Trained for 23 epochs.\n",
      "Done with 2477. Trained for 23 epochs.\n",
      "Done with 2718. Trained for 21 epochs.\n",
      "Done with 2983. Trained for 20 epochs.\n",
      "Done with 3274. Trained for 20 epochs.\n",
      "Done with 3593. Trained for 19 epochs.\n",
      "Done with 3944. Trained for 18 epochs.\n",
      "Done with 4328. Trained for 17 epochs.\n",
      "Done with 4750. Trained for 17 epochs.\n",
      "Done with 5214. Trained for 16 epochs.\n",
      "Done with 5722. Trained for 16 epochs.\n",
      "Done with 6280. Trained for 15 epochs.\n",
      "Done with 6892. Trained for 15 epochs.\n",
      "Done with 7564. Trained for 14 epochs.\n",
      "Done with 8302. Trained for 14 epochs.\n",
      "Done with 9111. Trained for 14 epochs.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "    \n",
    "    aligner, epoch = train_neural_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_neural_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    print(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce7558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural model, 10000 samples got a PSNR of 23.24\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "aligner = _LinearAlignment(resolution**2)\n",
    "\n",
    "for n_samples in [10000]:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_neural_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    print(f\"Neural model, {n_samples} samples got a PSNR of {validation_vectorized(aligned_model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a182d8b",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210c4b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 6892. Trained for 63 epochs.\n",
      "Done with 7564. Trained for 38 epochs.\n",
      "Done with 8302. Trained for 91 epochs.\n",
      "Done with 9111. Trained for 46 epochs.\n",
      "Done with 10000. Trained for 45 epochs.\n"
     ]
    }
   ],
   "source": [
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in samples_sets[79:]:\n",
    "    \n",
    "    aligner, epoch = train_mlp_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_mlp_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    print(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b4f91d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model, 1 samples got a PSNR of 11.55\n",
      "MLP model, 2 samples got a PSNR of 12.05\n",
      "MLP model, 3 samples got a PSNR of 12.53\n",
      "MLP model, 4 samples got a PSNR of 12.50\n",
      "MLP model, 5 samples got a PSNR of 12.69\n",
      "MLP model, 6 samples got a PSNR of 13.42\n",
      "MLP model, 7 samples got a PSNR of 13.11\n",
      "MLP model, 8 samples got a PSNR of 13.30\n",
      "MLP model, 9 samples got a PSNR of 13.63\n",
      "MLP model, 10 samples got a PSNR of 13.89\n",
      "MLP model, 11 samples got a PSNR of 13.67\n",
      "MLP model, 12 samples got a PSNR of 13.82\n",
      "MLP model, 13 samples got a PSNR of 14.23\n",
      "MLP model, 14 samples got a PSNR of 14.57\n",
      "MLP model, 16 samples got a PSNR of 14.82\n",
      "MLP model, 17 samples got a PSNR of 15.12\n",
      "MLP model, 19 samples got a PSNR of 15.29\n",
      "MLP model, 21 samples got a PSNR of 15.21\n",
      "MLP model, 23 samples got a PSNR of 15.30\n",
      "MLP model, 25 samples got a PSNR of 15.61\n",
      "MLP model, 28 samples got a PSNR of 15.55\n",
      "MLP model, 31 samples got a PSNR of 15.64\n",
      "MLP model, 34 samples got a PSNR of 15.80\n",
      "MLP model, 37 samples got a PSNR of 15.93\n",
      "MLP model, 41 samples got a PSNR of 16.40\n",
      "MLP model, 45 samples got a PSNR of 16.23\n",
      "MLP model, 49 samples got a PSNR of 16.55\n",
      "MLP model, 54 samples got a PSNR of 16.52\n",
      "MLP model, 59 samples got a PSNR of 16.66\n",
      "MLP model, 65 samples got a PSNR of 16.97\n",
      "MLP model, 72 samples got a PSNR of 17.07\n",
      "MLP model, 79 samples got a PSNR of 17.20\n",
      "MLP model, 86 samples got a PSNR of 17.52\n",
      "MLP model, 95 samples got a PSNR of 17.66\n",
      "MLP model, 104 samples got a PSNR of 17.79\n",
      "MLP model, 114 samples got a PSNR of 18.03\n",
      "MLP model, 126 samples got a PSNR of 18.38\n",
      "MLP model, 138 samples got a PSNR of 17.61\n",
      "MLP model, 151 samples got a PSNR of 18.56\n",
      "MLP model, 166 samples got a PSNR of 18.90\n",
      "MLP model, 183 samples got a PSNR of 19.23\n",
      "MLP model, 200 samples got a PSNR of 19.27\n",
      "MLP model, 220 samples got a PSNR of 19.33\n",
      "MLP model, 242 samples got a PSNR of 19.92\n",
      "MLP model, 265 samples got a PSNR of 19.28\n",
      "MLP model, 291 samples got a PSNR of 20.19\n",
      "MLP model, 319 samples got a PSNR of 20.55\n",
      "MLP model, 351 samples got a PSNR of 20.94\n",
      "MLP model, 385 samples got a PSNR of 20.15\n",
      "MLP model, 422 samples got a PSNR of 21.30\n",
      "MLP model, 464 samples got a PSNR of 21.36\n",
      "MLP model, 509 samples got a PSNR of 21.61\n",
      "MLP model, 559 samples got a PSNR of 21.86\n",
      "MLP model, 613 samples got a PSNR of 21.90\n",
      "MLP model, 673 samples got a PSNR of 22.07\n",
      "MLP model, 739 samples got a PSNR of 22.10\n",
      "MLP model, 811 samples got a PSNR of 22.08\n",
      "MLP model, 890 samples got a PSNR of 22.49\n",
      "MLP model, 977 samples got a PSNR of 22.34\n",
      "MLP model, 1072 samples got a PSNR of 18.86\n",
      "MLP model, 1176 samples got a PSNR of 22.66\n",
      "MLP model, 1291 samples got a PSNR of 22.74\n",
      "MLP model, 1417 samples got a PSNR of 22.81\n",
      "MLP model, 1555 samples got a PSNR of 22.60\n",
      "MLP model, 1707 samples got a PSNR of 22.91\n",
      "MLP model, 1873 samples got a PSNR of 22.52\n",
      "MLP model, 2056 samples got a PSNR of 22.47\n",
      "MLP model, 2257 samples got a PSNR of 22.73\n",
      "MLP model, 2477 samples got a PSNR of 23.27\n",
      "MLP model, 2718 samples got a PSNR of 22.96\n",
      "MLP model, 2983 samples got a PSNR of 22.70\n",
      "MLP model, 3274 samples got a PSNR of 22.28\n",
      "MLP model, 3593 samples got a PSNR of 23.27\n",
      "MLP model, 3944 samples got a PSNR of 22.81\n",
      "MLP model, 4328 samples got a PSNR of 23.24\n",
      "MLP model, 4750 samples got a PSNR of 23.32\n",
      "MLP model, 5214 samples got a PSNR of 19.51\n",
      "MLP model, 5722 samples got a PSNR of 18.17\n",
      "MLP model, 6280 samples got a PSNR of 23.75\n",
      "MLP model, 6892 samples got a PSNR of 23.29\n",
      "MLP model, 7564 samples got a PSNR of 23.05\n",
      "MLP model, 8302 samples got a PSNR of 23.22\n",
      "MLP model, 9111 samples got a PSNR of 23.38\n",
      "MLP model, 10000 samples got a PSNR of 22.17\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "aligner = _MLPAlignment(input_dim=resolution**2, hidden_dims=[resolution**2])\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_mlp_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    print(f\"MLP model, {n_samples} samples got a PSNR of {validation_vectorized(aligned_model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6147f",
   "metadata": {},
   "source": [
    "# Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0209c166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1. Trained for 521 epochs.\n",
      "Done with 2. Trained for 610 epochs.\n",
      "Done with 3. Trained for 507 epochs.\n",
      "Done with 4. Trained for 527 epochs.\n",
      "Done with 5. Trained for 686 epochs.\n",
      "Done with 6. Trained for 641 epochs.\n",
      "Done with 7. Trained for 758 epochs.\n",
      "Done with 8. Trained for 763 epochs.\n",
      "Done with 9. Trained for 827 epochs.\n",
      "Done with 10. Trained for 881 epochs.\n",
      "Done with 11. Trained for 704 epochs.\n",
      "Done with 12. Trained for 686 epochs.\n",
      "Done with 13. Trained for 1173 epochs.\n",
      "Done with 14. Trained for 829 epochs.\n",
      "Done with 16. Trained for 1070 epochs.\n",
      "Done with 17. Trained for 830 epochs.\n",
      "Done with 19. Trained for 1045 epochs.\n",
      "Done with 21. Trained for 1029 epochs.\n",
      "Done with 23. Trained for 875 epochs.\n",
      "Done with 25. Trained for 944 epochs.\n",
      "Done with 28. Trained for 815 epochs.\n",
      "Done with 31. Trained for 1073 epochs.\n",
      "Done with 34. Trained for 878 epochs.\n",
      "Done with 37. Trained for 1134 epochs.\n",
      "Done with 41. Trained for 1242 epochs.\n",
      "Done with 45. Trained for 1147 epochs.\n",
      "Done with 49. Trained for 1318 epochs.\n",
      "Done with 54. Trained for 1429 epochs.\n",
      "Done with 59. Trained for 1087 epochs.\n",
      "Done with 65. Trained for 860 epochs.\n",
      "Done with 72. Trained for 858 epochs.\n",
      "Done with 79. Trained for 972 epochs.\n",
      "Done with 86. Trained for 748 epochs.\n",
      "Done with 95. Trained for 803 epochs.\n",
      "Done with 104. Trained for 965 epochs.\n",
      "Done with 114. Trained for 857 epochs.\n",
      "Done with 126. Trained for 1226 epochs.\n",
      "Done with 138. Trained for 387 epochs.\n",
      "Done with 151. Trained for 483 epochs.\n",
      "Done with 166. Trained for 317 epochs.\n",
      "Done with 183. Trained for 740 epochs.\n",
      "Done with 200. Trained for 258 epochs.\n",
      "Done with 220. Trained for 283 epochs.\n",
      "Done with 242. Trained for 698 epochs.\n",
      "Done with 265. Trained for 227 epochs.\n",
      "Done with 291. Trained for 428 epochs.\n",
      "Done with 319. Trained for 598 epochs.\n",
      "Done with 351. Trained for 368 epochs.\n",
      "Done with 385. Trained for 262 epochs.\n",
      "Done with 422. Trained for 369 epochs.\n",
      "Done with 464. Trained for 389 epochs.\n",
      "Done with 509. Trained for 423 epochs.\n",
      "Done with 559. Trained for 519 epochs.\n",
      "Done with 613. Trained for 351 epochs.\n",
      "Done with 673. Trained for 428 epochs.\n",
      "Done with 739. Trained for 390 epochs.\n",
      "Done with 811. Trained for 271 epochs.\n",
      "Done with 890. Trained for 330 epochs.\n",
      "Done with 977. Trained for 325 epochs.\n",
      "Done with 1072. Trained for 250 epochs.\n",
      "Done with 1176. Trained for 304 epochs.\n",
      "Done with 1291. Trained for 264 epochs.\n",
      "Done with 1417. Trained for 259 epochs.\n",
      "Done with 1555. Trained for 199 epochs.\n",
      "Done with 1707. Trained for 190 epochs.\n",
      "Done with 1873. Trained for 198 epochs.\n",
      "Done with 2056. Trained for 234 epochs.\n",
      "Done with 2257. Trained for 182 epochs.\n",
      "Done with 2477. Trained for 198 epochs.\n",
      "Done with 2718. Trained for 190 epochs.\n",
      "Done with 2983. Trained for 182 epochs.\n",
      "Done with 3274. Trained for 158 epochs.\n",
      "Done with 3593. Trained for 164 epochs.\n",
      "Done with 3944. Trained for 169 epochs.\n",
      "Done with 4328. Trained for 131 epochs.\n",
      "Done with 4750. Trained for 130 epochs.\n",
      "Done with 5214. Trained for 129 epochs.\n",
      "Done with 5722. Trained for 101 epochs.\n",
      "Done with 6280. Trained for 119 epochs.\n",
      "Done with 6892. Trained for 117 epochs.\n",
      "Done with 7564. Trained for 125 epochs.\n",
      "Done with 8302. Trained for 99 epochs.\n",
      "Done with 9111. Trained for 112 epochs.\n",
      "Done with 10000. Trained for 72 epochs.\n"
     ]
    }
   ],
   "source": [
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "\n",
    "    aligner, epoch = train_conv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_conv_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    print(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee3286ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv model, 1 samples got a PSNR of 23.19\n",
      "Conv model, 2 samples got a PSNR of 33.89\n",
      "Conv model, 3 samples got a PSNR of 35.18\n",
      "Conv model, 4 samples got a PSNR of 36.29\n",
      "Conv model, 5 samples got a PSNR of 37.16\n",
      "Conv model, 6 samples got a PSNR of 37.53\n",
      "Conv model, 7 samples got a PSNR of 38.59\n",
      "Conv model, 8 samples got a PSNR of 38.29\n",
      "Conv model, 9 samples got a PSNR of 37.73\n",
      "Conv model, 10 samples got a PSNR of 38.61\n",
      "Conv model, 11 samples got a PSNR of 38.64\n",
      "Conv model, 12 samples got a PSNR of 38.68\n",
      "Conv model, 13 samples got a PSNR of 40.05\n",
      "Conv model, 14 samples got a PSNR of 39.49\n",
      "Conv model, 16 samples got a PSNR of 39.78\n",
      "Conv model, 17 samples got a PSNR of 39.39\n",
      "Conv model, 19 samples got a PSNR of 39.65\n",
      "Conv model, 21 samples got a PSNR of 39.40\n",
      "Conv model, 23 samples got a PSNR of 39.20\n",
      "Conv model, 25 samples got a PSNR of 39.47\n",
      "Conv model, 28 samples got a PSNR of 39.25\n",
      "Conv model, 31 samples got a PSNR of 39.68\n",
      "Conv model, 34 samples got a PSNR of 39.82\n",
      "Conv model, 37 samples got a PSNR of 39.83\n",
      "Conv model, 41 samples got a PSNR of 40.28\n",
      "Conv model, 45 samples got a PSNR of 40.40\n",
      "Conv model, 49 samples got a PSNR of 40.31\n",
      "Conv model, 54 samples got a PSNR of 40.75\n",
      "Conv model, 59 samples got a PSNR of 40.39\n",
      "Conv model, 65 samples got a PSNR of 40.46\n",
      "Conv model, 72 samples got a PSNR of 40.83\n",
      "Conv model, 79 samples got a PSNR of 40.88\n",
      "Conv model, 86 samples got a PSNR of 40.68\n",
      "Conv model, 95 samples got a PSNR of 40.90\n",
      "Conv model, 104 samples got a PSNR of 41.00\n",
      "Conv model, 114 samples got a PSNR of 41.07\n",
      "Conv model, 126 samples got a PSNR of 41.37\n",
      "Conv model, 138 samples got a PSNR of 40.21\n",
      "Conv model, 151 samples got a PSNR of 40.70\n",
      "Conv model, 166 samples got a PSNR of 39.82\n",
      "Conv model, 183 samples got a PSNR of 41.26\n",
      "Conv model, 200 samples got a PSNR of 39.94\n",
      "Conv model, 220 samples got a PSNR of 39.93\n",
      "Conv model, 242 samples got a PSNR of 41.34\n",
      "Conv model, 265 samples got a PSNR of 37.56\n",
      "Conv model, 291 samples got a PSNR of 41.17\n",
      "Conv model, 319 samples got a PSNR of 41.40\n",
      "Conv model, 351 samples got a PSNR of 40.79\n",
      "Conv model, 385 samples got a PSNR of 39.82\n",
      "Conv model, 422 samples got a PSNR of 41.21\n",
      "Conv model, 464 samples got a PSNR of 40.78\n",
      "Conv model, 509 samples got a PSNR of 41.46\n",
      "Conv model, 559 samples got a PSNR of 41.21\n",
      "Conv model, 613 samples got a PSNR of 41.36\n",
      "Conv model, 673 samples got a PSNR of 40.94\n",
      "Conv model, 739 samples got a PSNR of 41.39\n",
      "Conv model, 811 samples got a PSNR of 41.36\n",
      "Conv model, 890 samples got a PSNR of 41.24\n",
      "Conv model, 977 samples got a PSNR of 41.09\n",
      "Conv model, 1072 samples got a PSNR of 41.30\n",
      "Conv model, 1176 samples got a PSNR of 41.52\n",
      "Conv model, 1291 samples got a PSNR of 41.11\n",
      "Conv model, 1417 samples got a PSNR of 41.38\n",
      "Conv model, 1555 samples got a PSNR of 41.30\n",
      "Conv model, 1707 samples got a PSNR of 41.38\n",
      "Conv model, 1873 samples got a PSNR of 41.43\n",
      "Conv model, 2056 samples got a PSNR of 40.93\n",
      "Conv model, 2257 samples got a PSNR of 41.44\n",
      "Conv model, 2477 samples got a PSNR of 40.87\n",
      "Conv model, 2718 samples got a PSNR of 41.31\n",
      "Conv model, 2983 samples got a PSNR of 41.52\n",
      "Conv model, 3274 samples got a PSNR of 41.65\n",
      "Conv model, 3593 samples got a PSNR of 41.68\n",
      "Conv model, 3944 samples got a PSNR of 41.48\n",
      "Conv model, 4328 samples got a PSNR of 41.44\n",
      "Conv model, 4750 samples got a PSNR of 41.39\n",
      "Conv model, 5214 samples got a PSNR of 41.26\n",
      "Conv model, 5722 samples got a PSNR of 41.39\n",
      "Conv model, 6280 samples got a PSNR of 41.34\n",
      "Conv model, 6892 samples got a PSNR of 41.75\n",
      "Conv model, 7564 samples got a PSNR of 40.90\n",
      "Conv model, 8302 samples got a PSNR of 41.45\n",
      "Conv model, 9111 samples got a PSNR of 41.47\n",
      "Conv model, 10000 samples got a PSNR of 41.43\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "aligner = _ConvolutionalAlignment(in_channels=2*c, out_channels=2*c, kernel_size=5)\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_conv_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    print(f\"Conv model, {n_samples} samples got a PSNR of {validation_vectorized(aligned_model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1de4a",
   "metadata": {},
   "source": [
    "# Two Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 10. Trained for 382 epochs.\n"
     ]
    }
   ],
   "source": [
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "\n",
    "    aligner, epoch = train_twoconv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_twoconv_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    print(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twpconv model, 10 samples got a PSNR of 27.64\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "aligner = _TwoConvAlignment(in_channels=2*c, hidden_channels=2*c, out_channels=2*c, kernel_size=5)\n",
    "\n",
    "for n_samples in samples_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_twoconv_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    print(f\"Twpconv model, {n_samples} samples got a PSNR of {validation_vectorized(aligned_model, test_loader, times, device):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a2ce7",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01102be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 2.\n",
      "Done with 3.\n",
      "Done with 4.\n",
      "Done with 5.\n",
      "Done with 6.\n",
      "Done with 7.\n",
      "Done with 8.\n",
      "Done with 9.\n",
      "Done with 10.\n",
      "Done with 11.\n",
      "Done with 12.\n",
      "Done with 13.\n",
      "Done with 14.\n",
      "Done with 16.\n",
      "Done with 17.\n",
      "Done with 19.\n",
      "Done with 21.\n",
      "Done with 23.\n",
      "Done with 25.\n",
      "Done with 28.\n",
      "Done with 31.\n",
      "Done with 34.\n",
      "Done with 37.\n",
      "Done with 41.\n",
      "Done with 45.\n",
      "Done with 49.\n",
      "Done with 54.\n",
      "Done with 59.\n",
      "Done with 65.\n",
      "Done with 72.\n",
      "Done with 79.\n",
      "Done with 86.\n",
      "Done with 95.\n",
      "Done with 104.\n",
      "Done with 114.\n",
      "Done with 126.\n",
      "Done with 138.\n",
      "Done with 151.\n",
      "Done with 166.\n",
      "Done with 183.\n",
      "Done with 200.\n",
      "Done with 220.\n",
      "Done with 242.\n",
      "Done with 265.\n",
      "Done with 291.\n",
      "Done with 319.\n",
      "Done with 351.\n",
      "Done with 385.\n",
      "Done with 422.\n",
      "Done with 464.\n",
      "Done with 509.\n",
      "Done with 559.\n",
      "Done with 613.\n",
      "Done with 673.\n",
      "Done with 739.\n",
      "Done with 811.\n",
      "Done with 890.\n",
      "Done with 977.\n",
      "Done with 1072.\n",
      "Done with 1176.\n",
      "Done with 1291.\n",
      "Done with 1417.\n",
      "Done with 1555.\n",
      "Done with 1707.\n",
      "Done with 1873.\n",
      "Done with 2056.\n",
      "Done with 2257.\n",
      "Done with 2477.\n",
      "Done with 2718.\n",
      "Done with 2983.\n",
      "Done with 3274.\n",
      "Done with 3593.\n",
      "Done with 3944.\n",
      "Done with 4328.\n",
      "Done with 4750.\n",
      "Done with 5214.\n",
      "Done with 5722.\n",
      "Done with 6280.\n",
      "Done with 6892.\n",
      "Done with 7564.\n",
      "Done with 8302.\n",
      "Done with 9111.\n",
      "Done with 10000.\n"
     ]
    }
   ],
   "source": [
    "data.flat = True\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in samples_sets[1:]:\n",
    "\n",
    "    aligner = train_zeroshot_aligner(data, permutation, n_samples, train_snr, n_samples, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_zeroshot_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    print(f\"Done with {n_samples}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f0bef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroshot model, 2 samples got a PSNR of 11.23\n",
      "Zeroshot model, 3 samples got a PSNR of 11.66\n",
      "Zeroshot model, 4 samples got a PSNR of 12.17\n",
      "Zeroshot model, 5 samples got a PSNR of 12.97\n",
      "Zeroshot model, 6 samples got a PSNR of 13.13\n",
      "Zeroshot model, 7 samples got a PSNR of 13.40\n",
      "Zeroshot model, 8 samples got a PSNR of 13.53\n",
      "Zeroshot model, 9 samples got a PSNR of 14.24\n",
      "Zeroshot model, 10 samples got a PSNR of 14.29\n",
      "Zeroshot model, 11 samples got a PSNR of 14.90\n",
      "Zeroshot model, 12 samples got a PSNR of 15.14\n",
      "Zeroshot model, 13 samples got a PSNR of 15.20\n",
      "Zeroshot model, 14 samples got a PSNR of 15.26\n",
      "Zeroshot model, 16 samples got a PSNR of 15.43\n",
      "Zeroshot model, 17 samples got a PSNR of 15.52\n",
      "Zeroshot model, 19 samples got a PSNR of 15.57\n",
      "Zeroshot model, 21 samples got a PSNR of 15.93\n",
      "Zeroshot model, 23 samples got a PSNR of 15.88\n",
      "Zeroshot model, 25 samples got a PSNR of 16.30\n",
      "Zeroshot model, 28 samples got a PSNR of 16.55\n",
      "Zeroshot model, 31 samples got a PSNR of 16.73\n",
      "Zeroshot model, 34 samples got a PSNR of 16.85\n",
      "Zeroshot model, 37 samples got a PSNR of 16.98\n",
      "Zeroshot model, 41 samples got a PSNR of 17.17\n",
      "Zeroshot model, 45 samples got a PSNR of 17.38\n",
      "Zeroshot model, 49 samples got a PSNR of 17.61\n",
      "Zeroshot model, 54 samples got a PSNR of 17.83\n",
      "Zeroshot model, 59 samples got a PSNR of 18.09\n",
      "Zeroshot model, 65 samples got a PSNR of 18.33\n",
      "Zeroshot model, 72 samples got a PSNR of 18.58\n",
      "Zeroshot model, 79 samples got a PSNR of 18.87\n",
      "Zeroshot model, 86 samples got a PSNR of 18.94\n",
      "Zeroshot model, 95 samples got a PSNR of 19.29\n",
      "Zeroshot model, 104 samples got a PSNR of 19.56\n",
      "Zeroshot model, 114 samples got a PSNR of 19.86\n",
      "Zeroshot model, 126 samples got a PSNR of 20.17\n",
      "Zeroshot model, 138 samples got a PSNR of 20.47\n",
      "Zeroshot model, 151 samples got a PSNR of 20.66\n",
      "Zeroshot model, 166 samples got a PSNR of 21.04\n",
      "Zeroshot model, 183 samples got a PSNR of 21.41\n",
      "Zeroshot model, 200 samples got a PSNR of 21.77\n",
      "Zeroshot model, 220 samples got a PSNR of 22.10\n",
      "Zeroshot model, 242 samples got a PSNR of 22.16\n",
      "Zeroshot model, 265 samples got a PSNR of 22.69\n",
      "Zeroshot model, 291 samples got a PSNR of 23.30\n",
      "Zeroshot model, 319 samples got a PSNR of 22.41\n",
      "Zeroshot model, 351 samples got a PSNR of 24.15\n",
      "Zeroshot model, 385 samples got a PSNR of 24.61\n",
      "Zeroshot model, 422 samples got a PSNR of 25.01\n",
      "Zeroshot model, 464 samples got a PSNR of 25.63\n",
      "Zeroshot model, 509 samples got a PSNR of 26.15\n",
      "Zeroshot model, 559 samples got a PSNR of 26.69\n",
      "Zeroshot model, 613 samples got a PSNR of 27.21\n",
      "Zeroshot model, 673 samples got a PSNR of 26.06\n",
      "Zeroshot model, 739 samples got a PSNR of 28.38\n",
      "Zeroshot model, 811 samples got a PSNR of 29.10\n",
      "Zeroshot model, 890 samples got a PSNR of 29.81\n",
      "Zeroshot model, 977 samples got a PSNR of 30.47\n",
      "Zeroshot model, 1072 samples got a PSNR of 31.18\n",
      "Zeroshot model, 1176 samples got a PSNR of 31.80\n",
      "Zeroshot model, 1291 samples got a PSNR of 32.29\n",
      "Zeroshot model, 1417 samples got a PSNR of 33.26\n",
      "Zeroshot model, 1555 samples got a PSNR of 33.95\n",
      "Zeroshot model, 1707 samples got a PSNR of 34.61\n",
      "Zeroshot model, 1873 samples got a PSNR of 35.13\n",
      "Zeroshot model, 2056 samples got a PSNR of 35.62\n",
      "Zeroshot model, 2257 samples got a PSNR of 36.21\n",
      "Zeroshot model, 2477 samples got a PSNR of 36.60\n",
      "Zeroshot model, 2718 samples got a PSNR of 37.40\n",
      "Zeroshot model, 2983 samples got a PSNR of 37.12\n",
      "Zeroshot model, 3274 samples got a PSNR of 37.95\n",
      "Zeroshot model, 3593 samples got a PSNR of 38.15\n",
      "Zeroshot model, 3944 samples got a PSNR of 37.67\n",
      "Zeroshot model, 4328 samples got a PSNR of 37.74\n",
      "Zeroshot model, 4750 samples got a PSNR of 38.57\n",
      "Zeroshot model, 5214 samples got a PSNR of 38.68\n",
      "Zeroshot model, 5722 samples got a PSNR of 38.37\n",
      "Zeroshot model, 6280 samples got a PSNR of 38.11\n",
      "Zeroshot model, 6892 samples got a PSNR of 38.96\n",
      "Zeroshot model, 7564 samples got a PSNR of 38.64\n",
      "Zeroshot model, 8302 samples got a PSNR of 38.71\n",
      "Zeroshot model, 9111 samples got a PSNR of 38.39\n",
      "Zeroshot model, 10000 samples got a PSNR of 38.45\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "for n_samples in samples_sets[1:]:\n",
    "\n",
    "    aligner = _ZeroShotAlignment(\n",
    "        F_tilde=torch.zeros(n_samples, resolution**2),\n",
    "        G_tilde=torch.zeros(resolution**2, n_samples), \n",
    "        G=torch.zeros(1, 1),\n",
    "        L=torch.zeros(n_samples, n_samples),\n",
    "        mean=torch.zeros(n_samples, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_zeroshot_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    print(f\"Zeroshot model, {n_samples} samples got a PSNR of {validation_vectorized(aligned_model, test_loader, times, device):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
