{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e4bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lorenzo/repos/Deep-JSCC-PyTorch\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from alignment.alignment_utils import load_deep_jscc\n",
    "from alignment.alignment_model import *\n",
    "from alignment.alignment_model import _LinearAlignment, _MLPAlignment, _ConvolutionalAlignment, _ZeroShotAlignment, _TwoConvAlignment\n",
    "from alignment.alignment_training import *\n",
    "from alignment.alignment_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4dce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching inputs: 100%|██████████| 49/49 [00:02<00:00, 18.08it/s]\n"
     ]
    }
   ],
   "source": [
    "snr = 30\n",
    "seed = 42\n",
    "resolution = 96\n",
    "\n",
    "model1_fp = f'alignment/models/autoencoders/snr_{snr}_seed_42.pkl'\n",
    "model2_fp = f'alignment/models/autoencoders/snr_{snr}_seed_43.pkl'\n",
    "folder = f'psnr_vs_pilots'\n",
    "os.makedirs(f'alignment/models/plots/{folder}', exist_ok=True)\n",
    "\n",
    "dataset = \"cifar10\"\n",
    "channel = 'AWGN'\n",
    "batch_size = 1024\n",
    "num_workers = 4\n",
    "\n",
    "logs_folder = f'alignment/logs_{resolution}'\n",
    "os.makedirs(logs_folder, exist_ok=True)\n",
    "\n",
    "train_snr = snr\n",
    "val_snr = snr\n",
    "times = 10\n",
    "c = 8\n",
    "\n",
    "n_points = 20\n",
    "pilots_sets = np.unique(np.logspace(0, np.log10(10000), num=n_points, base=10).astype(int))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = copy.deepcopy(load_deep_jscc(model1_fp, val_snr, c, \"AWGN\").encoder)\n",
    "decoder = copy.deepcopy(load_deep_jscc(model2_fp, val_snr, c, \"AWGN\").decoder)\n",
    "\n",
    "train_loader, test_loader = get_data_loaders(dataset, resolution, batch_size, num_workers)\n",
    "data = load_alignment_dataset(model1_fp, model2_fp, train_snr, train_loader, c, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c117ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 13/19 [14:48<06:50, 68.37s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m permutation = torch.randperm(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_samples \u001b[38;5;129;01min\u001b[39;00m tqdm(pilots_sets, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     aligner, epoch = \u001b[43mtrain_neural_aligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_snr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     aligner_fp = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33malignment/models/plots/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/aligner_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maligner_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     14\u001b[39m     torch.save(aligner.state_dict(), aligner_fp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/Deep-JSCC-PyTorch/alignment/alignment_training.py:256\u001b[39m, in \u001b[36mtrain_neural_aligner\u001b[39m\u001b[34m(data, permutation, n_samples, batch_size, resolution, ratio, train_snr, device)\u001b[39m\n\u001b[32m    254\u001b[39m     loss.backward()\n\u001b[32m    255\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# validation phase\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_val:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for seed in [42, 43, 44, 45, 46]:\n",
    "\n",
    "    aligner_type = \"neural\"\n",
    "    data.flat = False\n",
    "    \n",
    "    set_seed(seed)\n",
    "    permutation = torch.randperm(len(data))\n",
    "    \n",
    "    for n_samples in tqdm(pilots_sets, desc=\"Training\"):\n",
    "        \n",
    "        aligner, epoch = train_neural_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device)\n",
    "    \n",
    "        aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "        torch.save(aligner.state_dict(), aligner_fp)\n",
    "    \n",
    "    aligner = _LinearAlignment(resolution**2)\n",
    "    log_file = f\"alignment/logs/aligner_{aligner_type}_snr_{snr}_seed_{seed}.txt\"\n",
    "    \n",
    "    with open(log_file, 'w') as f:\n",
    "        pass\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    for n_samples in pilots_sets:\n",
    "    \n",
    "        aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "        aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "    \n",
    "        aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "    \n",
    "        psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "        \n",
    "        result_msg = f\"Neural model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "        print(result_msg)\n",
    "        \n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(f\"{result_msg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea546b",
   "metadata": {},
   "source": [
    "# No mismatch - Unaligned - Zeroshot max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unaligned 11.72\n",
      "aligned 43.71\n",
      "zeroshot 27.91\n"
     ]
    }
   ],
   "source": [
    "log_file = f\"{logs_folder}/lines_snr_{snr}_seed_{seed}.txt\"\n",
    "\n",
    "# unaligned\n",
    "model = AlignedDeepJSCC(encoder, decoder, None, val_snr, \"AWGN\")\n",
    "\n",
    "result_msg = f\"unaligned {validation_vectorized(model, test_loader, times, device):.2f}\"\n",
    "print(result_msg)\n",
    "with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")\n",
    "\n",
    "# aligned\n",
    "model = AlignedDeepJSCC(encoder, copy.deepcopy(load_deep_jscc(model1_fp, val_snr, c, \"AWGN\").decoder), None, val_snr, \"AWGN\")\n",
    "\n",
    "result_msg = f\"aligned {validation_vectorized(model, test_loader, times, device):.2f}\"\n",
    "print(result_msg)\n",
    "with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")\n",
    "\n",
    "# zeroshot\n",
    "data.flat = True\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "aligner = train_zeroshot_aligner(data, permutation, resolution**2, train_snr, resolution**2, device)\n",
    "aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "result_msg = f\"zeroshot {validation_vectorized(aligned_model, test_loader, times, device):.2f}\"\n",
    "print(result_msg)\n",
    "with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29cfc6",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039882c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"linear\"\n",
    "data.flat = True\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm(pilots_sets, desc=\"Training\"):\n",
    "\n",
    "    aligner = train_linear_aligner(data, permutation, n_samples, train_snr)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    # tqdm.write(f\"Done with {n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dd758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model, 10000 samples got a PSNR of 42.61\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"linear\"\n",
    "aligner = _LinearAlignment(resolution**2)\n",
    "log_file = f\"{logs_folder}/aligner_{aligner_type}_snr_{snr}_seed_{seed}.txt\"\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    pass\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in pilots_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, channel)\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Linear model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f355d4",
   "metadata": {},
   "source": [
    "# Linear Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_aligner(data, permutation, n_samples, batch_size, resolution, ratio, train_snr, device):\n",
    "    \"\"\"\n",
    "    Train convolutional aligner with Adam optimization using train/validation split,\n",
    "    mixed precision, warmup, and learning rate scheduling.\n",
    "    \"\"\"\n",
    "\n",
    "    import torch.cuda.amp as amp\n",
    "\n",
    "    # train settings\n",
    "    epochs_max = 10000\n",
    "    patience = 20\n",
    "    min_delta = 1e-5\n",
    "    base_lr = 1e-3     # max learning rate\n",
    "    final_lr = 1e-4    # target low LR\n",
    "    warmup_frac = 0.05\n",
    "\n",
    "    # prepare data with train/validation split\n",
    "    indices = permutation[:n_samples]\n",
    "\n",
    "    if n_samples < 10:\n",
    "        use_val = False\n",
    "        train_indices = indices\n",
    "        val_indices = []\n",
    "    else:\n",
    "        use_val = True\n",
    "        val_size = max(1, int(0.1 * n_samples))\n",
    "        train_size = n_samples - val_size\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "\n",
    "    # create datasets and dataloaders\n",
    "    train_subset = AlignmentSubset(data, train_indices)\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if use_val:\n",
    "        val_subset = AlignmentSubset(data, val_indices)\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # prepare model and optimizer\n",
    "    aligner = _LinearAlignment(size=resolution * resolution * 3 * 2 // ratio).to(device)\n",
    "    channel = Channel(\"AWGN\", train_snr)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.Adam(aligner.parameters(), lr=base_lr)\n",
    "\n",
    "    # learning rate scheduler with warmup\n",
    "    total_steps = epochs_max * len(train_dataloader)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=base_lr,\n",
    "        steps_per_epoch=len(train_dataloader),\n",
    "        epochs=epochs_max,\n",
    "        final_div_factor=base_lr / final_lr,\n",
    "        pct_start=warmup_frac,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    # AMP scaler for mixed precision\n",
    "    scaler = amp.GradScaler('cuda')\n",
    "\n",
    "    # init train state\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    checks_without_improvement = 0\n",
    "    epoch = 0\n",
    "\n",
    "    while True:\n",
    "        aligner.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            if train_snr is not None:\n",
    "                inputs = channel(inputs)\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with amp.autocast('cuda'):\n",
    "                outputs = aligner(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss_scaled = loss * inputs.shape[0]\n",
    "\n",
    "            scaler.scale(loss_scaled).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss_scaled.item()\n",
    "\n",
    "        # validation phase\n",
    "        if use_val:\n",
    "            aligner.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_dataloader:\n",
    "                    if train_snr is not None:\n",
    "                        inputs = channel(inputs)\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    with amp.autocast('cuda'):\n",
    "                        outputs = aligner(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        loss_scaled = loss * inputs.shape[0]\n",
    "                        val_loss += loss_scaled.item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            current_loss = avg_val_loss\n",
    "        else:\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            current_loss = avg_train_loss\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        if best_loss - current_loss > min_delta:\n",
    "            best_loss = current_loss\n",
    "            best_model_state = copy.deepcopy(aligner.state_dict())\n",
    "            checks_without_improvement = 0\n",
    "        else:\n",
    "            checks_without_improvement += 1\n",
    "\n",
    "        if checks_without_improvement >= patience or epoch > epochs_max:\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "\n",
    "    return aligner.cpu(), epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_43437/3908411140.py:58: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n",
      "/tmp/ipykernel_43437/3908411140.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "/home/lorenzo/miniconda3/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_43437/3908411140.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"neural\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm([10000], desc=\"Training\"):\n",
    "    \n",
    "    aligner, epoch = train_neural_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "aligner = _LinearAlignment(resolution**2)\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in [10000]:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Neural model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979343a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19/19 [07:41<00:00, 24.29s/it]\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"neural\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm(pilots_sets, desc=\"Training\"):\n",
    "    \n",
    "    aligner, epoch = train_neural_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    # tqdm.write(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce7558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural model, 1 samples got a PSNR of 12.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural model, 2 samples got a PSNR of 12.78\n",
      "Neural model, 4 samples got a PSNR of 12.06\n",
      "Neural model, 6 samples got a PSNR of 12.69\n",
      "Neural model, 11 samples got a PSNR of 12.92\n",
      "Neural model, 18 samples got a PSNR of 13.27\n",
      "Neural model, 29 samples got a PSNR of 13.52\n",
      "Neural model, 48 samples got a PSNR of 13.96\n",
      "Neural model, 78 samples got a PSNR of 14.55\n",
      "Neural model, 127 samples got a PSNR of 15.00\n",
      "Neural model, 206 samples got a PSNR of 15.41\n",
      "Neural model, 335 samples got a PSNR of 15.98\n",
      "Neural model, 545 samples got a PSNR of 16.68\n",
      "Neural model, 885 samples got a PSNR of 18.00\n",
      "Neural model, 1438 samples got a PSNR of 18.58\n",
      "Neural model, 2335 samples got a PSNR of 20.09\n",
      "Neural model, 3792 samples got a PSNR of 21.40\n",
      "Neural model, 6158 samples got a PSNR of 23.84\n",
      "Neural model, 10000 samples got a PSNR of 25.22\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"neural\"\n",
    "aligner = _LinearAlignment(resolution**2)\n",
    "log_file = f\"{logs_folder}/aligner_{aligner_type}_snr_{snr}_seed_{seed}.txt\"\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    pass\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in pilots_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Neural model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a182d8b",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02efa31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_aligner(data, permutation, n_samples, batch_size, resolution, ratio, train_snr, device):\n",
    "    \"\"\"\n",
    "    Train convolutional aligner with Adam optimization using train/validation split.\n",
    "    \"\"\"\n",
    "\n",
    "    # train settings\n",
    "    epochs_max=1000\n",
    "    patience=20\n",
    "    min_delta=1e-5\n",
    "\n",
    "    # prepare data with train/validation split\n",
    "    indices = permutation[:n_samples]\n",
    "    \n",
    "    # handle small datasets (< 10 samples)\n",
    "    if n_samples < 10:\n",
    "        use_val = False\n",
    "\n",
    "        # use all data for training, no validation split\n",
    "        train_indices = indices\n",
    "        val_indices = []\n",
    "\n",
    "    else:\n",
    "        use_val = True\n",
    "\n",
    "        # split into 90 train - 10 validation\n",
    "        val_size = max(1, int(0.1 * n_samples))\n",
    "        train_size = n_samples - val_size\n",
    "        \n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "    \n",
    "    # create datasets and dataloaders\n",
    "    train_subset = AlignmentSubset(data, train_indices)\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    if use_val:\n",
    "        val_subset = AlignmentSubset(data, val_indices)\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # prepare model and optimizer\n",
    "    size = resolution * resolution * 3 * 2 // ratio\n",
    "    aligner = _MLPAlignment(size, [size]).to(device)\n",
    "    channel = Channel(\"AWGN\", train_snr)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.Adam(aligner.parameters(), lr=1e-3)\n",
    "    \n",
    "    # add learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "    # init train state\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    checks_without_improvement = 0\n",
    "    epoch = 0\n",
    "\n",
    "    # train loop\n",
    "    while True:\n",
    "        # training phase\n",
    "        aligner.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            if train_snr is not None:\n",
    "                inputs = channel(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = aligner(inputs.to(device))\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "            loss = loss * inputs.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # validation phase\n",
    "        if use_val:\n",
    "            aligner.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_dataloader:\n",
    "                    if train_snr is not None:\n",
    "                        inputs = channel(inputs)\n",
    "                    \n",
    "                    outputs = aligner(inputs.to(device))\n",
    "                    loss = criterion(outputs, targets.to(device))\n",
    "                    loss = loss * inputs.shape[0]\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            # use validation loss for early stopping\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            current_loss = avg_val_loss\n",
    "        else:\n",
    "            # use training loss if no validation set\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            current_loss = avg_train_loss\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        # step the scheduler with current loss\n",
    "        scheduler.step(current_loss)\n",
    "\n",
    "        # check if improvement\n",
    "        if best_loss - current_loss > min_delta:\n",
    "            best_loss = current_loss\n",
    "            best_model_state = copy.deepcopy(aligner.state_dict())\n",
    "            checks_without_improvement = 0\n",
    "        else:\n",
    "            checks_without_improvement += 1\n",
    "\n",
    "        # break if patience exceeded\n",
    "        if checks_without_improvement >= patience:\n",
    "            break\n",
    "\n",
    "        # break if max epochs exceeded\n",
    "        if epoch > epochs_max:\n",
    "            break\n",
    "\n",
    "    # restore best model\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "    \n",
    "    return aligner.cpu(), epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3b4575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [19:33<00:00, 1173.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model, 10000 samples got a PSNR of 17.56\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"mlp\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm([10000], desc=\"Training\"):\n",
    "    \n",
    "    aligner, epoch = train_mlp_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "aligner = _MLPAlignment(input_dim=resolution**2, hidden_dims=[resolution**2])\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in [10000]:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"MLP model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner_type = \"mlp\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for reg in [0.0001, 0.001, 0.01]:\n",
    "    for lr in [1e-5, 1e-4, 1e-3]:\n",
    "\n",
    "        for n_samples in tqdm([10000], desc=\"Training\"):\n",
    "            \n",
    "            aligner, epoch = train_mlp_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device, lr, reg)\n",
    "\n",
    "            aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "            torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "        aligner = _MLPAlignment(input_dim=resolution**2, hidden_dims=[resolution**2])\n",
    "\n",
    "        set_seed(seed)\n",
    "\n",
    "        for n_samples in [10000]:\n",
    "\n",
    "            aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "            aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "            aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "            psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "            \n",
    "            result_msg = f\"LR: {lr} REG: {reg}, MLP model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "            print(result_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c4b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [02:20<00:00, 140.62s/it]\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"mlp\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm(pilots_sets, desc=\"Training\"):\n",
    "    \n",
    "    aligner, epoch = train_mlp_aligner(data, permutation, n_samples, batch_size, resolution, 6, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    # tqdm.write(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f91d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model, 10000 samples got a PSNR of 27.07\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"mlp\"\n",
    "aligner = _MLPAlignment(input_dim=resolution**2, hidden_dims=[resolution**2])\n",
    "log_file = f\"{logs_folder}/aligner_{aligner_type}_snr_{snr}_seed_{seed}.txt\"\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    pass\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in pilots_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"MLP model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6147f",
   "metadata": {},
   "source": [
    "# Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e33b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device):\n",
    "    \"\"\"\n",
    "    Train convolutional aligner with Adam optimization using train/validation split.\n",
    "    \"\"\"\n",
    "\n",
    "    # train settings\n",
    "    epochs_max=10000\n",
    "    patience=10\n",
    "    min_delta=1e-5\n",
    "    reg_val = 0.001\n",
    "\n",
    "    # prepare data with train/validation split\n",
    "    indices = permutation[:n_samples]\n",
    "    \n",
    "    # handle small datasets (< 10 samples)\n",
    "    if n_samples < 10:\n",
    "        use_val = False\n",
    "\n",
    "        # use all data for training, no validation split\n",
    "        train_indices = indices\n",
    "        val_indices = []\n",
    "\n",
    "    else:\n",
    "        use_val = True\n",
    "\n",
    "        # split into 90 train - 10 validation\n",
    "        val_size = max(1, int(0.1 * n_samples))\n",
    "        train_size = n_samples - val_size\n",
    "        \n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "    \n",
    "    # create datasets and dataloaders\n",
    "    train_subset = AlignmentSubset(data, train_indices)\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    if use_val:\n",
    "        val_subset = AlignmentSubset(data, val_indices)\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # prepare model and optimizer\n",
    "    aligner = _ConvolutionalAlignment(in_channels=2*c, out_channels=2*c, kernel_size=5).to(device)\n",
    "    channel = Channel(\"AWGN\", train_snr)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.Adam(aligner.parameters(), lr=1e-4)\n",
    "\n",
    "    # init train state\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    checks_without_improvement = 0\n",
    "    epoch = 0\n",
    "\n",
    "    # train loop\n",
    "    while True:\n",
    "        # training phase\n",
    "        aligner.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            if train_snr is not None:\n",
    "                inputs = channel(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = aligner(inputs.to(device))\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "            loss = loss * inputs.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # validation phase\n",
    "        if use_val:\n",
    "            aligner.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_dataloader:\n",
    "                    if train_snr is not None:\n",
    "                        inputs = channel(inputs)\n",
    "                    \n",
    "                    outputs = aligner(inputs.to(device))\n",
    "                    loss = criterion(outputs, targets.to(device))\n",
    "                    loss = loss * inputs.shape[0]\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            # use validation loss for early stopping\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            current_loss = avg_val_loss\n",
    "        else:\n",
    "            # use training loss if no validation set\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            current_loss = avg_train_loss\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        # check if improvement\n",
    "        if best_loss - current_loss > min_delta:\n",
    "            best_loss = current_loss\n",
    "            best_model_state = copy.deepcopy(aligner.state_dict())\n",
    "            checks_without_improvement = 0\n",
    "        else:\n",
    "            checks_without_improvement += 1\n",
    "\n",
    "        # break if patience exceeded\n",
    "        if checks_without_improvement >= patience:\n",
    "            break\n",
    "\n",
    "        # break if max epochs exceeded\n",
    "        if epoch > epochs_max:\n",
    "            break\n",
    "\n",
    "    # restore best model\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "    \n",
    "    return aligner.cpu(), epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30cdcbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [04:32<00:00, 272.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv model, 10000 samples got a PSNR of 41.66\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"conv\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm([10000], desc=\"Training\"):\n",
    "    \n",
    "    aligner, epoch = train_conv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "aligner = _ConvolutionalAlignment(in_channels=2*c, out_channels=2*c, kernel_size=5)\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in [10000]:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Conv model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209c166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19/19 [03:37<00:00, 11.45s/it]\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"conv\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm(pilots_sets, desc=\"Training\"):\n",
    "\n",
    "    aligner, epoch = train_conv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    # tqdm.write(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3286ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv model, 1 samples got a PSNR of 24.77\n",
      "Conv model, 2 samples got a PSNR of 30.57\n",
      "Conv model, 4 samples got a PSNR of 32.55\n",
      "Conv model, 6 samples got a PSNR of 33.52\n",
      "Conv model, 11 samples got a PSNR of 32.98\n",
      "Conv model, 18 samples got a PSNR of 34.33\n",
      "Conv model, 29 samples got a PSNR of 34.56\n",
      "Conv model, 48 samples got a PSNR of 34.82\n",
      "Conv model, 78 samples got a PSNR of 34.81\n",
      "Conv model, 127 samples got a PSNR of 35.05\n",
      "Conv model, 206 samples got a PSNR of 35.31\n",
      "Conv model, 335 samples got a PSNR of 35.41\n",
      "Conv model, 545 samples got a PSNR of 35.34\n",
      "Conv model, 885 samples got a PSNR of 35.34\n",
      "Conv model, 1438 samples got a PSNR of 35.35\n",
      "Conv model, 2335 samples got a PSNR of 35.33\n",
      "Conv model, 3792 samples got a PSNR of 35.40\n",
      "Conv model, 6158 samples got a PSNR of 35.26\n",
      "Conv model, 10000 samples got a PSNR of 35.22\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"conv\"\n",
    "aligner = _ConvolutionalAlignment(in_channels=2*c, out_channels=2*c, kernel_size=5)\n",
    "log_file = f\"{logs_folder}/aligner_{aligner_type}_snr_{snr}_seed_{seed}.txt\"\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    pass\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in pilots_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Conv model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1de4a",
   "metadata": {},
   "source": [
    "# Two Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ad7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_twoconv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device):\n",
    "    \"\"\"\n",
    "    Train convolutional aligner with Adam optimization using train/validation split.\n",
    "    \"\"\"\n",
    "\n",
    "    # train settings\n",
    "    epochs_max=10000\n",
    "    patience=20\n",
    "    min_delta=1e-5\n",
    "    # prepare data with train/validation split\n",
    "    indices = permutation[:n_samples]\n",
    "    \n",
    "    # handle small datasets (< 10 samples)\n",
    "    if n_samples < 10:\n",
    "        use_val = False\n",
    "\n",
    "        # use all data for training, no validation split\n",
    "        train_indices = indices\n",
    "        val_indices = []\n",
    "\n",
    "    else:\n",
    "        use_val = True\n",
    "\n",
    "        # split into 90 train - 10 validation\n",
    "        val_size = max(1, int(0.1 * n_samples))\n",
    "        train_size = n_samples - val_size\n",
    "        \n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "    \n",
    "    # create datasets and dataloaders\n",
    "    train_subset = AlignmentSubset(data, train_indices)\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    if use_val:\n",
    "        val_subset = AlignmentSubset(data, val_indices)\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # prepare model and optimizer\n",
    "    aligner = _TwoConvAlignment(in_channels=2*c, hidden_channels=2*c, out_channels=2*c, kernel_size=5).to(device)\n",
    "    channel = Channel(\"AWGN\", train_snr)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.Adam(aligner.parameters(), lr=1e-4)\n",
    "\n",
    "    # init train state\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    checks_without_improvement = 0\n",
    "    epoch = 0\n",
    "\n",
    "    # train loop\n",
    "    while True:\n",
    "        # training phase\n",
    "        aligner.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_dataloader:\n",
    "            if train_snr is not None:\n",
    "                inputs = channel(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = aligner(inputs.to(device))\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "            loss = loss * inputs.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # validation phase\n",
    "        if use_val:\n",
    "            aligner.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_dataloader:\n",
    "                    if train_snr is not None:\n",
    "                        inputs = channel(inputs)\n",
    "                    \n",
    "                    outputs = aligner(inputs.to(device))\n",
    "                    loss = criterion(outputs, targets.to(device))\n",
    "                    loss = loss * inputs.shape[0]\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            # use validation loss for early stopping\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            current_loss = avg_val_loss\n",
    "        else:\n",
    "            # use training loss if no validation set\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            current_loss = avg_train_loss\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        # check if improvement\n",
    "        if best_loss - current_loss > min_delta:\n",
    "            best_loss = current_loss\n",
    "            best_model_state = copy.deepcopy(aligner.state_dict())\n",
    "            checks_without_improvement = 0\n",
    "        else:\n",
    "            checks_without_improvement += 1\n",
    "\n",
    "        # break if patience exceeded\n",
    "        if checks_without_improvement >= patience:\n",
    "            break\n",
    "\n",
    "        # break if max epochs exceeded\n",
    "        if epoch > epochs_max:\n",
    "            break\n",
    "\n",
    "    # restore best model\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "    \n",
    "    return aligner.cpu(), epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9520b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [21:32<00:00, 1292.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twoconv model, 10000 samples got a PSNR of 44.29\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"twoconv\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm([10000], desc=\"Training\"):\n",
    "    \n",
    "    aligner, epoch = train_twoconv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "aligner = _TwoConvAlignment(in_channels=2*c, hidden_channels=2*c, out_channels=2*c, kernel_size=5)\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in [10000]:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Twoconv model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19/19 [16:56<00:00, 53.52s/it] \n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"twoconv\"\n",
    "data.flat = False\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm(pilots_sets, desc=\"Training\"):\n",
    "\n",
    "    aligner, epoch = train_twoconv_aligner(data, permutation, n_samples, c, batch_size, train_snr, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    # tqdm.write(f\"Done with {n_samples}. Trained for {epoch} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twoconv model, 1 samples got a PSNR of 16.13\n",
      "Twoconv model, 2 samples got a PSNR of 24.65\n",
      "Twoconv model, 4 samples got a PSNR of 30.05\n",
      "Twoconv model, 6 samples got a PSNR of 31.60\n",
      "Twoconv model, 11 samples got a PSNR of 32.94\n",
      "Twoconv model, 18 samples got a PSNR of 33.66\n",
      "Twoconv model, 29 samples got a PSNR of 35.74\n",
      "Twoconv model, 48 samples got a PSNR of 36.88\n",
      "Twoconv model, 78 samples got a PSNR of 37.24\n",
      "Twoconv model, 127 samples got a PSNR of 37.55\n",
      "Twoconv model, 206 samples got a PSNR of 37.68\n",
      "Twoconv model, 335 samples got a PSNR of 37.92\n",
      "Twoconv model, 545 samples got a PSNR of 37.86\n",
      "Twoconv model, 885 samples got a PSNR of 38.43\n",
      "Twoconv model, 1438 samples got a PSNR of 38.45\n",
      "Twoconv model, 2335 samples got a PSNR of 38.72\n",
      "Twoconv model, 3792 samples got a PSNR of 38.53\n",
      "Twoconv model, 6158 samples got a PSNR of 38.11\n",
      "Twoconv model, 10000 samples got a PSNR of 38.74\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"twoconv\"\n",
    "aligner = _TwoConvAlignment(in_channels=2*c, hidden_channels=2*c, out_channels=2*c, kernel_size=5)\n",
    "log_file = f\"{logs_folder}/aligner_{aligner_type}_snr_{snr}_seed_{seed}.txt\"\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    pass\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in pilots_sets:\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Twoconv model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a2ce7",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01102be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [01:31<00:00,  5.10s/it]\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"zeroshot\"\n",
    "data.flat = True\n",
    "\n",
    "set_seed(seed)\n",
    "permutation = torch.randperm(len(data))\n",
    "\n",
    "for n_samples in tqdm(pilots_sets[1:], desc=\"Training\"):\n",
    "\n",
    "    aligner = train_zeroshot_aligner(data, permutation, n_samples, train_snr, n_samples, device)\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    torch.save(aligner.state_dict(), aligner_fp)\n",
    "\n",
    "    # tqdm.write(f\"Done with {n_samples}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0bef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroshot model, 2 samples got a PSNR of 11.28\n",
      "Zeroshot model, 4 samples got a PSNR of 11.55\n",
      "Zeroshot model, 6 samples got a PSNR of 11.77\n",
      "Zeroshot model, 11 samples got a PSNR of 11.90\n",
      "Zeroshot model, 18 samples got a PSNR of 12.72\n",
      "Zeroshot model, 29 samples got a PSNR of 13.34\n",
      "Zeroshot model, 48 samples got a PSNR of 13.29\n",
      "Zeroshot model, 78 samples got a PSNR of 14.30\n",
      "Zeroshot model, 127 samples got a PSNR of 15.11\n",
      "Zeroshot model, 206 samples got a PSNR of 15.08\n",
      "Zeroshot model, 335 samples got a PSNR of 16.24\n",
      "Zeroshot model, 545 samples got a PSNR of 15.94\n",
      "Zeroshot model, 885 samples got a PSNR of 16.95\n",
      "Zeroshot model, 1438 samples got a PSNR of 19.43\n",
      "Zeroshot model, 2335 samples got a PSNR of 18.31\n",
      "Zeroshot model, 3792 samples got a PSNR of 22.53\n",
      "Zeroshot model, 6158 samples got a PSNR of 26.78\n",
      "Zeroshot model, 10000 samples got a PSNR of 28.33\n"
     ]
    }
   ],
   "source": [
    "aligner_type = \"zeroshot\"\n",
    "log_file = f\"{logs_folder}/aligner_{aligner_type}_snr_{snr}_seed_{seed}.txt\"\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    pass\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "for n_samples in pilots_sets[1:]:\n",
    "\n",
    "    aligner = _ZeroShotAlignment(\n",
    "        F_tilde=torch.zeros(n_samples, resolution**2),\n",
    "        G_tilde=torch.zeros(resolution**2, n_samples), \n",
    "        G=torch.zeros(1, 1),\n",
    "        L=torch.zeros(n_samples, n_samples),\n",
    "        mean=torch.zeros(n_samples, 1)\n",
    "    )\n",
    "\n",
    "    aligner_fp = f'alignment/models/plots/{folder}/aligner_{aligner_type}_{n_samples}.pth'\n",
    "    aligner.load_state_dict(torch.load(aligner_fp, map_location=device))\n",
    "\n",
    "    aligned_model = AlignedDeepJSCC(encoder, decoder, aligner, val_snr, \"AWGN\")\n",
    "\n",
    "    psnr_result = validation_vectorized(aligned_model, test_loader, times, device)\n",
    "    \n",
    "    result_msg = f\"Zeroshot model, {n_samples} samples got a PSNR of {psnr_result:.2f}\"\n",
    "    print(result_msg)\n",
    "    \n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"{result_msg}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
